{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load all libraries we need to have in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3744\\4105609030.py:4: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('cleaned_dataset.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   USMER  MEDICAL_UNIT  SEX  PATIENT_TYPE   DATE_DIED  INTUBED  PNEUMONIA  \\\n",
      "0      2             1    1             1  2020-03-05      2.0        1.0   \n",
      "1      2             1    2             1  2020-03-06      2.0        1.0   \n",
      "2      2             1    2             2  2020-09-06      1.0        2.0   \n",
      "3      2             1    1             1  2020-12-06      2.0        2.0   \n",
      "4      2             1    2             1  2020-06-21      2.0        2.0   \n",
      "\n",
      "    AGE  PREGNANT  DIABETES  ...  INMSUPR  HIPERTENSION  OTHER_DISEASE  \\\n",
      "0  65.0       2.0       2.0  ...      2.0           1.0            2.0   \n",
      "1  72.0       2.0       2.0  ...      2.0           1.0            2.0   \n",
      "2  55.0       2.0       1.0  ...      2.0           2.0            2.0   \n",
      "3  53.0       2.0       2.0  ...      2.0           2.0            2.0   \n",
      "4  68.0       2.0       1.0  ...      2.0           1.0            2.0   \n",
      "\n",
      "   CARDIOVASCULAR  OBESITY  RENAL_CHRONIC  TOBACCO  CLASIFFICATION_FINAL  ICU  \\\n",
      "0             2.0      2.0            2.0      2.0                     3  2.0   \n",
      "1             2.0      1.0            1.0      2.0                     5  2.0   \n",
      "2             2.0      2.0            2.0      2.0                     3  2.0   \n",
      "3             2.0      2.0            2.0      2.0                     7  2.0   \n",
      "4             2.0      2.0            2.0      2.0                     3  2.0   \n",
      "\n",
      "   DEAD  \n",
      "0     1  \n",
      "1     1  \n",
      "2     1  \n",
      "3     1  \n",
      "4     1  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV data\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Inspect the data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 . Preprocess the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['USMER', 'MEDICAL_UNIT', 'SEX', 'PATIENT_TYPE', 'DATE_DIED', 'INTUBED',\n",
      "       'PNEUMONIA', 'AGE', 'PREGNANT', 'DIABETES', 'COPD', 'ASTHMA', 'INMSUPR',\n",
      "       'HIPERTENSION', 'OTHER_DISEASE', 'CARDIOVASCULAR', 'OBESITY',\n",
      "       'RENAL_CHRONIC', 'TOBACCO', 'CLASIFFICATION_FINAL', 'ICU', 'DEAD'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3744\\4267729199.py:9: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('cleaned_dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv('cleaned_dataset.csv')\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X = df.drop(['AGE', 'DIABETES', 'DEAD'], axis=1)\n",
    "y = df['DEAD']\n",
    "\n",
    "# Identify numerical columns\n",
    "numeric_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Extract relevant features from the date column\n",
    "X['DATE_DIED'] = pd.to_datetime(X['DATE_DIED'])\n",
    "X['year'] = X['DATE_DIED'].dt.year\n",
    "X['month'] = X['DATE_DIED'].dt.month\n",
    "X['day'] = X['DATE_DIED'].dt.day\n",
    "\n",
    "# Drop the original date column\n",
    "X = X.drop(['DATE_DIED'], axis=1)\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X = pd.get_dummies(X, columns=['SEX'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify numerical columns after one-hot encoding\n",
    "numeric_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Create a column transformer to standardize numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols)\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20972/20972 [==============================] - 74s 3ms/step - loss: 0.2817 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 2/10\n",
      "20972/20972 [==============================] - 60s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 3/10\n",
      "20972/20972 [==============================] - 53s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 4/10\n",
      "20972/20972 [==============================] - 55s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 5/10\n",
      "20972/20972 [==============================] - 70s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 6/10\n",
      "20972/20972 [==============================] - 54s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 7/10\n",
      "20972/20972 [==============================] - 48s 2ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 8/10\n",
      "20972/20972 [==============================] - 45s 2ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 9/10\n",
      "20972/20972 [==============================] - 48s 2ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n",
      "Epoch 10/10\n",
      "20972/20972 [==============================] - 53s 3ms/step - loss: 0.2628 - accuracy: 0.9264 - val_loss: 0.2626 - val_accuracy: 0.9265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28385554700>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6554/6554 [==============================] - 12s 2ms/step - loss: 0.2605 - accuracy: 0.9273\n",
      "Test Accuracy: 92.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Fine-Tune and Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the initial results, we can fine-tune our model. Experiment with different architectures, activation functions, and hyperparameters. Consider using techniques like dropout or regularization to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save and Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Neural_Model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
